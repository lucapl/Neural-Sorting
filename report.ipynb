{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,roc_auc_score,accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input,Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Layer, Concatenate, Lambda, Normalization\n",
    "from tensorflow.keras.metrics import Metric, Precision, Recall, AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model,get_custom_objects\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car evaluation\n",
    "\n",
    "#### Łukasz Andryszewski 151930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used is the Car Evalutaion dataset which can be found [here](https://en.cs.uni-paderborn.de/is/research/research-projects/software/monotone-learning-datasets).\n",
    "\n",
    "It consists of six criterions and four classes. The criterions are:\n",
    "- price\n",
    "- price of the maintenance\n",
    "- number of doors\n",
    "- capacity \n",
    "- size of luggage boot\n",
    "- estimated safety\n",
    "\n",
    "The criteria are normalized between 0 and 1.\n",
    "\n",
    "Based on them the alternatives are assigned to four sorted classes, which are:\n",
    "1. unacceptable \n",
    "2. acceptable\n",
    "3. good\n",
    "4. very good\n",
    "\n",
    "However here they will be binerized between the second and third class into to:\n",
    "\n",
    "1. Bad\n",
    "2. Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/monodata/car evaluation.csv\",header=None)\n",
    "features = len(data.columns)-1\n",
    "crits = [\"price\",\"maintaince price\",\"doors\",\"capacity\",\"size of luggage\", \"safety\"]\n",
    "data.columns = crits+[\"class\"]#[f\"crit_{i}\" for i in range(features)]+[\"class\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"class\"]<=2,\"class\"] = 0\n",
    "data.loc[data[\"class\"]>=3,\"class\"] = 1\n",
    "data_classless = data.drop(columns=\"class\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(X,y):\n",
    "    cmap = [\"red\",\"blue\"]\n",
    "    #colors = tuple(map(lambda c:cmap[int(c)-1],data[\"class\"]))\n",
    "    #for i,col in enumerate(data.drop(columns=\"class\")):\n",
    "    features = X.shape[1]\n",
    "    fig,axs = plt.subplots(1,2,figsize=(10,6))\n",
    "    fig.tight_layout()\n",
    "    for i,x in enumerate(X):\n",
    "        \n",
    "        _y = np.copy(x)\n",
    "        _x = np.arange(features,dtype=np.float64)\n",
    "        c = int(y[i])\n",
    "        color = cmap[c]\n",
    "        #_y = np.copy(X[:,])\n",
    "        _y += np.random.normal(0,0.025,len(_y))\n",
    "        _y = np.clip(_y,0,1)\n",
    "        #_x = np.array([i for _ in range(len(_y))]).astype(np.float64)\n",
    "        _x += np.random.normal(0,0.1,len(_x))\n",
    "        axs[c].plot(_x,_y,alpha=0.05,color=color)\n",
    "        axs[c].set_ylabel(f\"Criterion value\")\n",
    "        axs[c].set_xlabel(f\"Criterions\")\n",
    "        axs[c].set_yticks(np.arange(1,step=0.0833333))\n",
    "        axs[c].set_xticks(np.arange(features,step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data(data_classless.to_numpy(),data[\"class\"].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the names and distribution of their values in different classes, it can be inferred that the price and maintaince price criterions are of cost types and the rest of the criterions are gain type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,count in Counter(data[\"class\"]).items():\n",
    "    print(f\"Class {c} occurences: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is highly imbalanced, so there is a need for undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_combs = 1\n",
    "for col in data_classless:\n",
    "    _combs *= len(np.unique(data[col]))\n",
    "\n",
    "print(\"Possible combinations of data:\",_combs)\n",
    "print(\"Number of alternatives:\",len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of possible combinations of all values of the criterions and the number of alternatives is the same. Judging by that suspicious fact, it is safe to assume that the dataset is composed of all possible alternatives or that there is quite a number of repeated alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of performance and to avoid learning the most of the space of alternatives the first class needs to be heavily undersampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(X,y,samples=None,class_0=0):\n",
    "    counts = Counter(y)\n",
    "    samples = min(counts.values()) if samples == None else samples\n",
    "    n = len(X)\n",
    "\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    for i in range(class_0,len(counts)+class_0):\n",
    "        ids = np.arange(len(y))\n",
    "        current_class = ids[y==i]\n",
    "\n",
    "        sampled = np.random.choice(current_class,samples)\n",
    "        new_X.append(X[sampled])\n",
    "        new_y.append(y[sampled])\n",
    "\n",
    "    new_data = np.column_stack((np.concatenate(new_X,axis=0),np.concatenate(new_y,axis=0)))\n",
    "\n",
    "    np.random.shuffle(new_data)\n",
    "\n",
    "    return new_data[:,:-1],new_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X,new_y = undersample(data_classless.to_numpy(),data[\"class\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data(new_X,new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rest, y_train, y_rest = train_test_split(new_X,new_y,test_size=0.40)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest,y_rest,test_size=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RankSVM method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate differencese between rows of different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_2d_differences(array:np.array):\n",
    "    difs = array[:,np.newaxis,:] - array \n",
    "    return difs.reshape(-1,array.shape[1])\n",
    "\n",
    "def calculate_1d_differences(vector:np.array):\n",
    "    difs = vector[:,np.newaxis] - vector\n",
    "    return difs.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_difs = calculate_2d_differences(X_train)\n",
    "X_test_difs = calculate_2d_differences(X_test)\n",
    "\n",
    "y_train_difs = calculate_1d_differences(y_train)\n",
    "y_test_difs = calculate_1d_differences(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_filtered = y_train_difs[y_train_difs != 0]\n",
    "y_test_filtered = y_test_difs[y_test_difs != 0]\n",
    "\n",
    "X_train_filtered = X_train_difs[y_train_difs != 0]\n",
    "X_test_filtered = X_test_difs[y_test_difs != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_svm = LinearSVC()#make_pipeline(StandardScaler(),LinearSVC())\n",
    "\n",
    "rank_svm.fit(X_train_filtered,y_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_stats(model,X,y):\n",
    "    y_pred = model.predict(X)\n",
    "    print(f\"Accuracy: {accuracy_score(y,y_pred):.4%}\")\n",
    "    print(f\"F1 score: {f1_score(y,y_pred):.4%}\")\n",
    "    auc = np.dot(X,model.coef_.T)\n",
    "    print(f\"AUC: {roc_auc_score(y,auc):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance on train set:\\n\")\n",
    "show_stats(rank_svm,X_train_filtered,y_train_filtered)\n",
    "print(\"\\nPerformance on test set:\\n\")\n",
    "show_stats(rank_svm,X_test_filtered,y_test_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + K.epsilon())) # epsilon to prevent zeroes\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,X_train,y_train,val_data=None,epochs=50,loss='categorical_crossentropy',patience=3):\n",
    "    early_stopping = EarlyStopping(monitor='loss' if val_data == None else \"val_loss\", patience=patience, restore_best_weights=True)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=loss,\n",
    "        metrics=[\"accuracy\",AUC(name=\"auc\"),F1Score()])\n",
    "\n",
    "    history = model.fit(X_train,y_train,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=val_data)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    metrics = [\"Accuracy\",\"F1_score\",\"AUC\"]\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1,figsize=(10, 8))\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.suptitle(\"Model metrics\", fontsize=24)\n",
    "\n",
    "    for ax,metric in zip(axs,metrics):\n",
    "\n",
    "        xy = history.history[metric.lower()]\n",
    "\n",
    "        ax.plot(xy, label=metric)\n",
    "        ax.plot(xy, label=metric)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN-UTADIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "def leaky_hard_sigmoid(x):\n",
    "    return K.switch(x < 0, \n",
    "                    x*delta, \n",
    "                    K.switch(\n",
    "                        x > 1,\n",
    "                        delta*(x-1) + 1,\n",
    "                        x\n",
    "                    ))\n",
    "\n",
    "#get_custom_objects().update({'leaky_hard_sigmoid': Activation(leaky_hard_sigmoid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonotoneBlock(Layer):\n",
    "    def __init__(self, units=1, branches=3, **kwargs):\n",
    "        assert branches >= 2\n",
    "        super(MonotoneBlock, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.branches = branches\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.sigmoids = [Dense(self.units,activation=Activation(leaky_hard_sigmoid)) for _ in range(self.branches)]\n",
    "        for sig in self.sigmoids:\n",
    "            sig.build(input_shape)\n",
    "        #self.sigmoids = Concatenate(axis=2)([sigs])\n",
    "        self.linear = Dense(self.units,activation=None)\n",
    "        self.linear.build((None,self.branches*self.units))\n",
    "        super(MonotoneBlock, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = K.concatenate([sig(inputs) for sig in self.sigmoids],axis=-1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MinMax(Layer):\n",
    "#     def __init__(self,ideal_alt,anti_ideal_alt,model,**kwargs):\n",
    "#         super(MinMax,self).__init__(**kwargs)\n",
    "#         self.ideal_alt = ideal_alt\n",
    "#         self.anti_ideal_alt = anti_ideal_alt\n",
    "\n",
    "#     def build(self,input_shape):\n",
    "#         super(MinMax,self).build(input_shape)\n",
    "\n",
    "#     def call(self,inputs):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MinMaxNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ideal = inputs[0]\n",
    "        anti = inputs[1]\n",
    "        inputs = inputs[1:]\n",
    "        normalized = (inputs - anti) / (ideal - anti + tf.keras.backend.epsilon())\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thresholder(Layer):\n",
    "    def __init__(self, thresholds, **kwargs):\n",
    "        super(Thresholder, self).__init__(**kwargs)\n",
    "        assert thresholds != None\n",
    "        # if thresholds[-1] != 1:\n",
    "        #     thresholds += [1]\n",
    "        self.thresholds = thresholds\n",
    "\n",
    "    def call(self, inputs):\n",
    "        indices = tf.argmax(tf.cast(tf.less(inputs, tf.expand_dims(self.thresholds, axis=0)), tf.float32), axis=1)\n",
    "        one_hot = tf.one_hot(indices, depth=len(self.thresholds)+1)\n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thresholder(Layer):\n",
    "    def __init__(self, thresholds, **kwargs):\n",
    "        super(Thresholder, self).__init__(**kwargs)\n",
    "        assert thresholds is not None\n",
    "        self.thresholds = thresholds\n",
    "\n",
    "    def call(self, inputs):\n",
    "        indices = tf.argmax(tf.cast(tf.less(inputs, tf.expand_dims(self.thresholds, axis=0)), tf.float32), axis=1)\n",
    "        one_hot = tf.one_hot(indices, depth=len(self.thresholds)+1)\n",
    "        return one_hot\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], len(self.thresholds) + 1)\n",
    "\n",
    "class MinMaxNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MinMaxNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ideal = inputs[0:1,:]  # Extract ideal from inputs\n",
    "        anti = inputs[1:2,:]    # Extract anti-ideal from inputs\n",
    "        actual_inputs = inputs[2:, :]  # Extract the rest of the inputs\n",
    "        normalized = (actual_inputs - anti) / (ideal - anti + tf.keras.backend.epsilon())\n",
    "        return normalized\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ann_utadis_model(thresholds,ideal_alt,anti_ideal_alt,L=3,n_criteria=features,n_labels=classes):\n",
    "    inputs = Input(shape=(n_criteria,))\n",
    "    #ideal_layer = Lambda(lambda x: tf.reshape(tf.constant(ideal_alt),(1,-1)))(inputs)\n",
    "    #anti_ideal_layer = Lambda(lambda x: tf.reshape(tf.constant(anti_ideal_alt),(1,-1)))(inputs)\n",
    "\n",
    "    #concat = Concatenate(axis=0)([inputs,ideal_layer,anti_ideal_layer])\n",
    "\n",
    "    def splitter(x) : \n",
    "        split = tf.split(x, n_criteria, 1)\n",
    "        return split    \n",
    "    \n",
    "    #split_layer = Lambda(splitter)(inputs)\n",
    "\n",
    "    splits = [Lambda(lambda x: x[:, i:i+1],name=f\"criteria_{i}\")(inputs) for i in range(n_criteria)]\n",
    "\n",
    "    monotones = [MonotoneBlock(branches=L)(split) for split in splits]\n",
    "    \n",
    "    concat = Concatenate(axis=1)(monotones)\n",
    "    linear = Dense(1,activation=None)(concat)\n",
    "    norm = Dense(4,activation=\"sigmoid\")(linear)\n",
    "\n",
    "    #norm = MinMaxNormalization()(linear)\n",
    "\n",
    "    #thresholder = Thresholder(thresholds)(norm)\n",
    "    #norm = Normalization()(linear)\n",
    "    \n",
    "    return Model(name=\"ann_utadis\",inputs=inputs,outputs = norm)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.25,0.5,0.75]\n",
    "ideal_alt = [0,0,1,1,1,1]\n",
    "antiideal_alt = [1,1,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uta_model = create_ann_utadis_model(thresholds,ideal_alt,antiideal_alt,5)\n",
    "uta_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(uta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(uta_model,X_train,y_train,val_data=(X_val,y_val),patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(features=features):\n",
    "\n",
    "    inputs = Input((features,))\n",
    "    layer = Dense(256,activation=\"relu\")(inputs)\n",
    "    layer = Dense(128,activation=\"relu\")(inputs)\n",
    "    layer = Dense(64,activation=\"relu\")(layer)\n",
    "    layer = Dense(32,activation=\"relu\")(layer)\n",
    "    outputs = Dense(4,activation=\"sigmoid\")(layer)\n",
    "\n",
    "    return Model(inputs=inputs,outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = create_nn_model(features)\n",
    "\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(nn_model,X_train,y_train,val_data=(X_val,y_val),patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--bibtex \n",
    "\n",
    "@Article{Tehrani2011/08,\n",
    "  title={Choquistic Regression: Generalizing Logistic Regression using the Choquet Integral},\n",
    "  author={Ali Fallah Tehrani and Weiwei Cheng and Eyke Hüllermeier},\n",
    "  year={2011/08},\n",
    "  booktitle={Proceedings of the 7th conference of the European Society for Fuzzy Logic and Technology (EUSFLAT-11)},\n",
    "  pages={868-875},\n",
    "  issn={1951-6851},\n",
    "  isbn={978-90-78677-00-0},\n",
    "  url={https://doi.org/10.2991/eusflat.2011.86},\n",
    "  doi={10.2991/eusflat.2011.86},\n",
    "  publisher={Atlantis Press}\n",
    "}\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
